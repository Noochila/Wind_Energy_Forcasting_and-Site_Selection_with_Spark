{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.feature import RFE\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load Data into Spark DataFrame\n",
    "df = spark.read.csv('../Dataset/Location1_preprocessed.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Separate the time column\n",
    "time_column = df.select('Time', 'Day')\n",
    "df_without_time = df.drop('Time', 'Day')\n",
    "\n",
    "# Step 4: Use MinMaxScaler to normalize the columns\n",
    "assembler = VectorAssembler(inputCols=df_without_time.columns, outputCol=\"features\")\n",
    "df_vectorized = assembler.transform(df_without_time)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_vectorized)\n",
    "df_normalized = scaler_model.transform(df_vectorized)\n",
    "\n",
    "# Step 5: Add the time column back to the normalized DataFrame\n",
    "df_normalized = df_normalized.join(time_column, how='inner')\n",
    "\n",
    "# Step 6: Define features (X) and target variable (y)\n",
    "rformula = RFormula(formula=\"Power ~ .\", featuresCol=\"features\", labelCol=\"label\")\n",
    "output = rformula.fit(df_normalized).transform(df_normalized)\n",
    "X = output.select(\"features\")\n",
    "y = output.select(\"label\")\n",
    "\n",
    "# Step 7: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = X.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 8: Create a machine learning model (Random Forest Regressor)\n",
    "model = RandomForestRegressor(numTrees=100, seed=42)\n",
    "\n",
    "# Step 9: Use Recursive Feature Elimination (RFE)\n",
    "rfe = RFE(estimator=model, numFeatures=3)\n",
    "model = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Step 10: Train the model on the selected features\n",
    "selected_features = model.getFeatureSubset()\n",
    "X_train_rfe = X_train.select(selected_features)\n",
    "X_test_rfe = X_test.select(selected_features)\n",
    "\n",
    "# Step 11: Fit the model\n",
    "model = model.estimator\n",
    "model_fit = model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Step 12: Make predictions on the test set\n",
    "predictions = model_fit.transform(X_test_rfe)\n",
    "\n",
    "# Step 13: Evaluate the model performance\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Step 14: Get the selected features\n",
    "selected_features = model.getFeatureSubset().toArray()\n",
    "selected_feature_names = X.columns[model.getFeatureSubset().toArray()]\n",
    "print(\"Selected Features:\")\n",
    "for feature_name in selected_feature_names:\n",
    "    print(feature_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
