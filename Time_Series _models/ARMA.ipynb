{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import ARMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TimeSeriesAnalysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.read.csv('/home/skystone/Documents/TimeSeries/T1.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "dataset = df.select(df[\"Date/Time\"].alias(\"timeStamp\"), df[\"Wind Speed (m/s)\"].alias(\"windSpeed\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit dataset to 5000 rows\n",
    "dataset = dataset.limit(5000)\n",
    "\n",
    "# Convert timeStamp format\n",
    "time_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "dataset = dataset.withColumn(\"timeStamp\", date_format(col(\"timeStamp\"), time_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timeStamp column to timestamp type\n",
    "dataset = dataset.withColumn(\"timeStamp\", dataset[\"timeStamp\"].cast(\"timestamp\"))\n",
    "\n",
    "# Set timeStamp as index\n",
    "dataset = dataset.withColumn(\"index\", col(\"timeStamp\").cast(\"long\")).drop(\"timeStamp\").sort(\"index\").drop(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values using forward fill\n",
    "windowSpec = Window.orderBy(\"index\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "dataset = dataset.withColumn(\"windSpeed_imputed\", col(\"windSpeed\").fillna(lag(\"windSpeed\").over(windowSpec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a column for ticks\n",
    "dataset = dataset.withColumn(\"Ticks\", (lag(\"windSpeed\").over(windowSpec)).isNull().cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original data\n",
    "original_data = dataset.toPandas()\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(original_data[\"Ticks\"], original_data[\"windSpeed_imputed\"])\n",
    "plt.xlabel(\"Ticks\")\n",
    "plt.ylabel(\"Wind Speed (m/s)\")\n",
    "plt.title(\"Original Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for stationarity check using ADF test\n",
    "def stationarity_check(df):\n",
    "    df_values = df.select(\"windSpeed_imputed\").toPandas()\n",
    "    result = adfuller(df_values[\"windSpeed_imputed\"])\n",
    "    print('Augmented Dickey-Fuller test:')\n",
    "    print(f'Test Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value}')\n",
    "\n",
    "# Apply stationarity check\n",
    "stationarity_check(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features\n",
    "for i in range(1, 5):\n",
    "    dataset = dataset.withColumn(f\"lag_{i}\", lag(\"windSpeed_imputed\", i).over(windowSpec))\n",
    "\n",
    "# Drop rows with missing lag features\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "feature_cols = [\"lag_1\", \"lag_2\", \"lag_3\", \"lag_4\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "dataset = assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ARMA model\n",
    "arma = ARMA(spark, labelCol=\"windSpeed_imputed\", featuresCol=\"features\", p=3, q=3)\n",
    "model = arma.fit(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "predictions = model.transform(dataset)\n",
    "predictions_pd = predictions.select(\"timeStamp\", \"prediction\").toPandas()\n",
    "plt.plot(original_data[\"Ticks\"], original_data[\"windSpeed_imputed\"], label=\"Actual\")\n",
    "plt.plot(predictions_pd[\"timeStamp\"], predictions_pd[\"prediction\"], label=\"Predicted\")\n",
    "plt.xlabel(\"Ticks\")\n",
    "plt.ylabel(\"Wind Speed (m/s)\")\n",
    "plt.title(\"Fitted data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"humidityModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate future dates\n",
    "future_dates = [pd.Timestamp(original_data[\"timeStamp\"].iloc[-1]) + pd.DateOffset(months=x) for x in range(1, 25)]\n",
    "future_datest_df = pd.DataFrame(index=future_dates[1:], columns=original_data.columns)\n",
    "future_df = pd.concat([original_data, future_datest_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for future dates\n",
    "future_df[\"forecast\"] = model.transform(assembler.transform(spark.createDataFrame(future_df))).select(\"prediction\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
